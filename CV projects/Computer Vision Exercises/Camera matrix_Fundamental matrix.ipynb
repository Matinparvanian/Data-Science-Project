{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb6261e",
   "metadata": {},
   "source": [
    "# Computer Vision, Fall 2022\n",
    "\n",
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beeea31",
   "metadata": {},
   "source": [
    "## Ex 5.1. Camera matrix (4 p)\n",
    "\n",
    "Using the focal length, principal point and skew coefficient values given in the template, and equations given in Lecture 6, slide 49, form the camera matrix P and compute image coordinates (x,y) for an object point X (0, 0, 1, 1) when the origin of the world coordinate frame is\n",
    "\n",
    "- exactly 3 meters away from the camera center, i.e. t = (0, 0, 3), and the camera is completely aligned with the world coordinate axis\n",
    "- exactly 5 meters away from the camera center, i.e. t = (0, 0, 5), and the camera is completely aligned with the world coordinate axis\n",
    "- t = (0.5, 1, 3) and the camera otherwise aligned with the world coordinate axis but it has only turned 20 degrees to the left\n",
    "- t = (15, 1, 3) and the camera otherwise aligned with the world coordinate axis but it has only turned 20 degrees to the left\n",
    "\n",
    "Discuss the phenomena behind the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dac225a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1:\n",
      "Camera Matrix P:\n",
      "[[7.19302047e+02 0.00000000e+00 3.34631235e+02 1.00389370e+03]\n",
      " [0.00000000e+00 7.18392548e+02 2.56166678e+02 7.68500033e+02]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 3.00000000e+00]]\n",
      "Image Coordinates (x, y): (334.63, 256.17)\n",
      "\n",
      "Scenario 2:\n",
      "Camera Matrix P:\n",
      "[[7.19302047e+02 0.00000000e+00 3.34631235e+02 1.67315617e+03]\n",
      " [0.00000000e+00 7.18392548e+02 2.56166678e+02 1.28083339e+03]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 5.00000000e+00]]\n",
      "Image Coordinates (x, y): (334.63, 256.17)\n",
      "\n",
      "Scenario 3:\n",
      "Camera Matrix P:\n",
      "[[ 7.19302047e+02  1.14450623e+02  3.14450502e+02  1.36354473e+03]\n",
      " [ 0.00000000e+00  7.62682340e+02 -4.98678549e+00  1.48689258e+03]\n",
      " [ 0.00000000e+00  3.42020143e-01  9.39692621e-01  3.00000000e+00]]\n",
      "Image Coordinates (x, y): (425.92, 376.15)\n",
      "\n",
      "Scenario 4:\n",
      "Camera Matrix P:\n",
      "[[ 7.19302047e+02  1.14450623e+02  3.14450502e+02  1.17934244e+04]\n",
      " [ 0.00000000e+00  7.62682340e+02 -4.98678549e+00  1.48689258e+03]\n",
      " [ 0.00000000e+00  3.42020143e-01  9.39692621e-01  3.00000000e+00]]\n",
      "Image Coordinates (x, y): (3073.30, 376.15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "# Focal length\n",
    "fc = [719.302047058490760, 718.392548175289110]\n",
    "\n",
    "# Principal point\n",
    "cc = [334.631234942930060, 256.166677783686790]\n",
    "\n",
    "# Skew coefficient\n",
    "alpha_c = 0.000000000000000\n",
    "\n",
    "\n",
    "# Your solution here\n",
    "# Object point\n",
    "X = np.array([0, 0, 1, 1])\n",
    "\n",
    "# Scenarios\n",
    "scenarios = [\n",
    "    {'t': np.array([0, 0, 3]), 'rotation_matrix': np.eye(3)},\n",
    "    {'t': np.array([0, 0, 5]), 'rotation_matrix': np.eye(3)},\n",
    "    {'t': np.array([0.5, 1, 3]), 'rotation_matrix': np.array([[1, 0, 0], [0, np.cos(np.radians(20)), -np.sin(np.radians(20))], [0, np.sin(np.radians(20)), np.cos(np.radians(20))]])},\n",
    "    {'t': np.array([15, 1, 3]), 'rotation_matrix': np.array([[1, 0, 0], [0, np.cos(np.radians(20)), -np.sin(np.radians(20))], [0, np.sin(np.radians(20)), np.cos(np.radians(20))]])}\n",
    "]\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    t = scenario['t']\n",
    "    R = scenario['rotation_matrix']\n",
    "\n",
    "    # Form the camera matrix P\n",
    "    K = np.array([[fc[0], alpha_c, cc[0]],\n",
    "                  [0, fc[1], cc[1]],\n",
    "                  [0, 0, 1]])\n",
    "    T = np.hstack((R, t.reshape(-1, 1)))\n",
    "    P = np.dot(K, T)\n",
    "\n",
    "    # Compute image coordinates (x, y)\n",
    "    Xc = np.dot(R, X[:3]) + t\n",
    "    x = (Xc[0] / Xc[2]) * fc[0] + cc[0]\n",
    "    y = (Xc[1] / Xc[2]) * fc[1] + cc[1]\n",
    "\n",
    "    print(f\"Scenario {i+1}:\")\n",
    "    print(\"Camera Matrix P:\")\n",
    "    print(P)\n",
    "    print(f\"Image Coordinates (x, y): ({x:.2f}, {y:.2f})\\n\")\n",
    "    \n",
    "#In Scenario 1 and Scenario 2, Camera Aligned with the World Coordinate Axis and at Different Distances.\n",
    "#The camera is aligned with the world coordinate axis and is positioned at\n",
    "#different distances from the object point (3 meters in Scenario 1 and 5 meters in Scenario 2). \n",
    "#Since the camera is aligned with the world coordinates and no rotation is applied, the image \n",
    "#coordinates remain constant. This demonstrates that the camera's position relative to the object\n",
    "#has a direct impact on the scaling of the image coordinates.\n",
    "\n",
    "#In Scenario 3 and Scenario 4,Camera Turned to the Left and at Different Distances .The camera is not \n",
    "#aligned with the world coordinate axis. It has been turned 20 degrees to the left. The effect\n",
    "#of the camera rotation is evident in these results. The image coordinates(x, y) are significantly\n",
    "#different from the previous scenarios. The impact of the rotation is more pronounced in Scenario 4, \n",
    "#where the camera is 15 meters away from the object.The camera rotation causes a change in\n",
    "#the perspective, leading to different image coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b4315",
   "metadata": {},
   "source": [
    "## Ex 5.2 Fundamental matrix (4 p)\n",
    "\n",
    "Compute the fundamental matrix F for the images image1.jpg and image2.jpg using the normalized 8-point algorithm. You may use ready made functions for detecting and matching the features, but develop the algorithm for deriving matrix F by yourself. Where are the epipoles for both images? Report both F and the two epipoles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f66169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fundamental Matrix F:\n",
      "[[-0.27143075 -0.4200483   0.445115  ]\n",
      " [-0.09650671 -0.16247703  0.16056809]\n",
      " [ 0.28130399  0.44379275 -0.46279422]]\n",
      "Epipole in Image 1:\n",
      "[-0.80295709 -0.10320342 -0.58703405]\n",
      "Epipole in Image 2:\n",
      "[0.56138292 0.44843658 0.69552416]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def F_8point(x1, x2):\n",
    "\n",
    "# Your solution here\n",
    "    # Normalize the points\n",
    "    x1_normalized = x1 / np.linalg.norm(x1, axis=1)[:, np.newaxis]\n",
    "    x2_normalized = x2 / np.linalg.norm(x2, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Construct the A matrix\n",
    "    A = np.column_stack((x1_normalized[:, 0] * x2_normalized[:, 0],\n",
    "                         x1_normalized[:, 0] * x2_normalized[:, 1],\n",
    "                         x1_normalized[:, 0],\n",
    "                         x1_normalized[:, 1] * x2_normalized[:, 0],\n",
    "                         x1_normalized[:, 1] * x2_normalized[:, 1],\n",
    "                         x1_normalized[:, 1],\n",
    "                         x2_normalized[:, 0],\n",
    "                         x2_normalized[:, 1],\n",
    "                         np.ones(len(x1_normalized))))\n",
    "\n",
    "    # Solve for the fundamental matrix using SVD\n",
    "    _, _, V = np.linalg.svd(A)\n",
    "    F = V[-1].reshape(3, 3)\n",
    "\n",
    "    # Enforce the rank-2 constraint on F\n",
    "    U, S, Vt = np.linalg.svd(F)\n",
    "    S[2] = 0\n",
    "    F = np.dot(U, np.dot(np.diag(S), Vt))\n",
    "\n",
    "    return F\n",
    "\n",
    "# Load images\n",
    "image1 = cv2.imread('image 1.jpg')\n",
    "image2 = cv2.imread('image 2.jpg')\n",
    "\n",
    "\n",
    "# Example using SIFT (replace with your preferred method):\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect key points and compute descriptors for both images\n",
    "kp1, des1 = sift.detectAndCompute(image1, None)\n",
    "kp2, des2 = sift.detectAndCompute(image2, None)\n",
    "\n",
    "# Perform feature matching\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# Apply ratio test to obtain good matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Get corresponding feature points\n",
    "x1 = np.array([kp1[match.queryIdx].pt + (1,) for match in good_matches])\n",
    "x2 = np.array([kp2[match.trainIdx].pt + (1,) for match in good_matches])\n",
    "\n",
    "# Compute the fundamental matrix F\n",
    "F = F_8point(x1, x2)\n",
    "\n",
    "# Compute the epipoles using SVD\n",
    "U, S, Vt = np.linalg.svd(F)\n",
    "epipole1 = Vt[-1]  \n",
    "epipole2 = U[:, -1] \n",
    "\n",
    "print(\"Fundamental Matrix F:\")\n",
    "print(F)\n",
    "print(\"Epipole in Image 1:\")\n",
    "print(epipole1)\n",
    "print(\"Epipole in Image 2:\")\n",
    "print(epipole2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
